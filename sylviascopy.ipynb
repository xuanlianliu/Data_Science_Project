{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6fa92a-fb5d-4bda-ba79-c8589c63bcc9",
   "metadata": {},
   "source": [
    "# __FINAL PROJECT PHASE II__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc71f8-040e-4b90-8ed7-cbea963dc1a6",
   "metadata": {},
   "source": [
    "# __RESEARCH QUESTION:__\n",
    "\n",
    "What set of criteria is most important to obtain the most viewership on Netflix for movies? Are we able to accurately predict viewership according to various observed criteria, including ratings, global availability, and genre?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d2ad4-4778-4e55-8427-4afedaa5950a",
   "metadata": {},
   "source": [
    "### Importing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dcf011d-20dc-48a4-b38d-8018f9449014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3413e42b-affb-4e5a-9d91-632a5a02bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: openpyxl in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\2950\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4b3df-c2f9-4a96-a29a-4172f535aca1",
   "metadata": {},
   "source": [
    "### Data Overview & Sources:\n",
    "\n",
    "Seven data tables were collected. The first five were taken from Netflix regarding semi-annual engagement reports starting from the first half of 2023 to the first half of 2025. Each report contains 2 tabs, Shows and Films, and their respective data (i.e. runtime, viewership, global availability). Meanwhile, the IMDb movie/shows data table displays all the movies and shows, each having their own identification tag. The IMDb rating table references these tags to provide each movie/show with their respective ratings and number of votes. The following two IMDb data tables were combined to give us an extensive IMDb table to cross reference with the Netflix reports.\n",
    "\n",
    "Source for Netflix Engagement Report First Half 2023: https://about.netflix.com/en/news/what-we-watched-a-netflix-engagement-report\n",
    "\n",
    "Source for Netflix Engagement Report Second Half 2023: https://about.netflix.com/en/news/what-we-watched-the-second-half-of-2023\n",
    "\n",
    "Source for Netflix Engagement Report First Half 2024: https://about.netflix.com/en/news/what-we-watched-the-first-half-of-2024\n",
    "\n",
    "Source for Netflix Engagement Report Second Half 2024: https://about.netflix.com/en/news/what-we-watched-the-second-half-of-2024\n",
    "\n",
    "Source for Netflix Engagement Report First Half 2025: https://about.netflix.com/en/news/what-we-watched-the-first-half-of-2025\n",
    "\n",
    "Source of IMDb Ratings for Movies/Shows: https://datasets.imdbws.com/title.ratings.tsv.gz\n",
    "\n",
    "Source of IMDb Movie/Show Titles: https://datasets.imdbws.com/title.basics.tsv.gz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c754bd-377d-48c8-84d5-006787c525ff",
   "metadata": {},
   "source": [
    "# __Data Collection & Cleaning:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6984b98-03ed-425b-b684-63cc6ad39003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset with all ratings from IMBb. \n",
    "#note that each rating has an ID, not the show/movie title\n",
    "ratings_df= pd.read_table(\"title.ratings.tsv\")\n",
    "\n",
    "#import the dataset with show/movie (episode) title given the ID\n",
    "titles_df= pd.read_table(\"title.basics.tsv\")\n",
    "\n",
    "#import the dataset with show/movie parent ID (for series titles), given the episode ID\n",
    "series_title_id_df = pd.read_table(\"title.episode.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae1feff-0021-4406-8a3d-7a4ac955c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform an SQL join to obtain a dataframe with the rating, number of votes for that rating,\n",
    "#title of the show/move, and the show/movie genre\n",
    "merged_ratings_df= duckdb.sql(\"\"\"SELECT r.tconst, r.averageRating, r.numVotes, t.genres,\n",
    "t.originalTitle AS Title\n",
    "FROM ratings_df r, titles_df t\n",
    "WHERE r.tconst=t.tconst\"\"\").df()\n",
    "\n",
    "#drop the duplicate titles to prevent duplicate rows when merging with Netflix dataframe\n",
    "merged_ratings_df = merged_ratings_df.drop_duplicates(subset=['Title'])\n",
    "\n",
    "#merged_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4c912d-721f-4d2a-b367-bbde4c7fd66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 1st half of 2023 netflix data\n",
    "all_jan_jun_2023= pd.read_excel(\"What_We_Watched_A_Netflix_Engagement_Report_2023Jan-Jun.xlsx\",\n",
    "                                  sheet_name=\"Engagement\",\n",
    "                                  skiprows=5)\n",
    "all_jan_jun_2023['Time_Period']= \"2023 H1\"\n",
    "#import 2nd half of 2023 netflix data\n",
    "movies_jul_dec_2023= pd.read_excel(\"What_We_Watched_A_Netflix_Engagement_Report_2023Jul-Dec.xlsx\",\n",
    "                                  sheet_name=\"Film\",\n",
    "                                  skiprows=5)\n",
    "movies_jul_dec_2023['Time_Period']= \"2023 H2\"\n",
    "#import the 1st half of 2024 netflix data\n",
    "movies_jan_jun_2024= pd.read_excel(\"What_We_Watched_A_Netflix_Engagement_Report_2024Jan-Jun.xlsx\",\n",
    "                                   sheet_name= \"Film\",\n",
    "                                   skiprows=5)\n",
    "movies_jan_jun_2024['Time_Period']= \"2024 H1\"\n",
    "#import the 2nd half of 2024 netflix data\n",
    "movies_jul_dec_2024= pd.read_excel(\"What_We_Watched_A_Netflix_Engagement_Report_2024Jul-Dec.xlsx\",\n",
    "                                   sheet_name= \"Film\",\n",
    "                                   skiprows=5)\n",
    "movies_jul_dec_2024['Time_Period']= \"2024 H2\"\n",
    "#import the 1st half of 2025 netflix data\n",
    "movies_jan_jun_2025= pd.read_excel(\"What_We_Watched_A_Netflix_Engagement_Report_2025Jan-Jun.xlsx\",\n",
    "                                   sheet_name= \"Movies\",\n",
    "                                   skiprows=5)\n",
    "movies_jan_jun_2025['Time_Period']= \"2025 H1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d606e8db-dc2f-4c6b-8096-658497773143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27609, 7)\n",
      "(18040, 7)\n",
      "(8674, 7)\n",
      "(54323, 7)\n"
     ]
    }
   ],
   "source": [
    "#write code to combine all these dataframes into 1 Netflix dataframe.\n",
    "#be careful that some of the column names are different in different years and depending on whether \n",
    "#it's a show or movie. (The 1st half of 2023 doesn't have as many columns)\n",
    "\n",
    "jul_dec_2023= movies_jul_dec_2023\n",
    "\n",
    "netflix2023= duckdb.sql(\"\"\"SELECT \n",
    "                    Title, \n",
    "                    \"Available Globally?\" AS Global, \n",
    "                    \"Release Date\" AS Release_Date, \n",
    "                    \"Hours Viewed\" AS Hours_Viewed, \n",
    "                    Runtime, Views, Time_Period\n",
    "                FROM jul_dec_2023 \n",
    "                UNION \n",
    "                SELECT\n",
    "                    Title, \n",
    "                    \"Available Globally?\" AS Global, \n",
    "                    \"Release Date\" AS Release_Date, \n",
    "                    \"Hours Viewed\" AS Hours_Viewed, \n",
    "                    NULL AS Runtime, \n",
    "                    NULL AS Views, Time_Period\n",
    "                FROM all_jan_jun_2023\"\"\").df()\n",
    "\n",
    "#print(len(all_jan_jun_2023) + len(shows_jul_dec_2023) + len(movies_jul_dec_2023))\n",
    "print(netflix2023.shape)\n",
    "\n",
    "\n",
    "jan_jun_2024= duckdb.sql(\"\"\"SELECT\n",
    "                                Title, \n",
    "                                \"Available Globally?\" AS Global, \n",
    "                                \"Release Date\" AS Release_Date, \n",
    "                                \"Hours Viewed\" AS Hours_Viewed, \n",
    "                                Runtime, Views, Time_Period\n",
    "                            FROM movies_jan_jun_2024\"\"\").df()\n",
    "\n",
    "jul_dec_2024= duckdb.sql(\"\"\"SELECT\n",
    "                                Title, \n",
    "                                \"Available Globally?\" AS Global, \n",
    "                                \"Release Date\" AS Release_Date, \n",
    "                                \"Hours Viewed\" AS Hours_Viewed, \n",
    "                                Runtime, Views, Time_Period\n",
    "                            FROM movies_jul_dec_2024\"\"\").df()\n",
    "\n",
    "netflix2024= duckdb.sql(\"\"\"SELECT * FROM jan_jun_2024 UNION ALL SELECT * FROM jul_dec_2024\"\"\").df()\n",
    "#print(len(shows_jan_jun_2024) + len(movies_jan_jun_2024) \n",
    "#+ len(shows_jul_dec_2024) + len(movies_jul_dec_2024))\n",
    "print(netflix2024.shape)\n",
    "\n",
    "\n",
    "netflix2025= duckdb.sql(\"\"\"SELECT\n",
    "                                Title, \n",
    "                                \"Available Globally?\" AS Global, \n",
    "                                \"Release Date\" AS Release_Date, \n",
    "                                \"Hours Viewed\" AS Hours_Viewed, \n",
    "                                Runtime, Views, Time_Period\n",
    "                            FROM movies_jan_jun_2025\"\"\").df()\n",
    "\n",
    "#print(len(shows_jan_jun_2025) + len(movies_jan_jun_2025))\n",
    "print(netflix2025.shape)\n",
    "\n",
    "netflix_df= duckdb.sql(\"\"\"SELECT * FROM netflix2023 UNION SELECT * FROM netflix2024 UNION SELECT * \n",
    "FROM netflix2025\"\"\").df()\n",
    "\n",
    "print(netflix_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faccb3bc-906a-4952-9162-ab389d4ea4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54323, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Global</th>\n",
       "      <th>Release_Date</th>\n",
       "      <th>Hours_Viewed</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Views</th>\n",
       "      <th>Time_Period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nonnas</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>109500000</td>\n",
       "      <td>1:54</td>\n",
       "      <td>57600000</td>\n",
       "      <td>2025 H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carry-On</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-13</td>\n",
       "      <td>95400000</td>\n",
       "      <td>2:00</td>\n",
       "      <td>47700000</td>\n",
       "      <td>2025 H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K.O.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>63100000</td>\n",
       "      <td>1:27</td>\n",
       "      <td>43500000</td>\n",
       "      <td>2025 H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Widow's Game</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>83900000</td>\n",
       "      <td>2:02</td>\n",
       "      <td>41300000</td>\n",
       "      <td>2025 H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucca's World</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>46600000</td>\n",
       "      <td>1:37</td>\n",
       "      <td>28800000</td>\n",
       "      <td>2025 H1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Title Global Release_Date  Hours_Viewed Runtime     Views  \\\n",
       "0          Nonnas    Yes   2025-05-09     109500000    1:54  57600000   \n",
       "1        Carry-On    Yes   2024-12-13      95400000    2:00  47700000   \n",
       "2            K.O.    Yes   2025-06-06      63100000    1:27  43500000   \n",
       "3  A Widow's Game    Yes   2025-05-30      83900000    2:02  41300000   \n",
       "4   Lucca's World    Yes   2025-01-31      46600000    1:37  28800000   \n",
       "\n",
       "  Time_Period  \n",
       "0     2025 H1  \n",
       "1     2025 H1  \n",
       "2     2025 H1  \n",
       "3     2025 H1  \n",
       "4     2025 H1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cut the title of netflix shows with foreign language titles, keeping the english title only\n",
    "mod_title= netflix_df['Title']\n",
    "mod_title= mod_title.str.replace(r\" \\/\\/.*\", \"\", regex=True)\n",
    "\n",
    "netflix_df['Title']= mod_title\n",
    "print(netflix_df.shape)\n",
    "netflix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e21b215-c685-4169-8165-80b80ae89eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54323, 12)\n"
     ]
    }
   ],
   "source": [
    "#left join Netflix and ratings datasets\n",
    "#we want to keep all the Netflix observations, or else the dataset would shrink too much\n",
    "#limitation that only exact titles are matched\n",
    "\n",
    "netflix_ratings_df= duckdb.sql(\"\"\"SELECT netflix_df.Title, netflix_df.Global, netflix_df.Release_Date, \n",
    "netflix_df.Hours_Viewed, netflix_df.Runtime, netflix_df.Views, netflix_df.Time_Period, \n",
    "merged_ratings_df.averageRating, merged_ratings_df.numVotes, merged_ratings_df.genres\n",
    "FROM netflix_df\n",
    "LEFT JOIN merged_ratings_df\n",
    "ON netflix_df.Title= merged_ratings_df.Title\"\"\").df()\n",
    "\n",
    "#create new columns to use in future analysis\n",
    "netflix_ratings_df[\"Is_Global\"] = netflix_ratings_df[\"Global\"].map({'Yes': 1, 'No': 0})\n",
    "netflix_ratings_df[\"Successful\"] = netflix_ratings_df[\"Views\"] > 50000000\n",
    "netflix_ratings_df[\"Successful\"] = netflix_ratings_df[\"Successful\"].map({True: int(1), False: int(0)})\n",
    "\n",
    "\n",
    "print(netflix_ratings_df.shape) #yes, the size is the same as the netflix only dataframe\n",
    "\n",
    "#save this as an intermediate dataset in our final submission\n",
    "#used below code to troubleshoot the merge being the wrong size\n",
    "#netflix_ratings_df.to_csv('my_dataframe.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e1117-eb10-4b76-acc0-6ce602711e7e",
   "metadata": {},
   "source": [
    "# __Data Description:__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e00c1a-ec2d-4798-bc1f-15dcc4651af7",
   "metadata": {},
   "source": [
    "Our Netflix dataframe was created by joining multiple Netflix engagement reports from different periods and merged with the IMDb ratings dataset to observe if any factors had the most influence on viewership for Netflix films and predict viewership for films outside of the dataset according to observed criteria. All datasets used during this phase and their creation were funded by either Netflix or IMDb. \n",
    "\n",
    "The dataset consists of 48,231 Netflix films watched from January 2023 to June 2025, along with their respective attributes in terms of global availability, release date, hours viewed, runtime, views, average IMDb rating, number of votes, and genres. \n",
    "\n",
    "Our original plan was to create a dataset containing both Netflix films and shows but upon merging with the IMDb dataset, we discovered that many shows were missing rating information and so we decided to focus solely on films. This meant reducing our dataset by a couple thousand rows but given that we are still able to work with around 48,000 instances of films, we should have sufficient data for performing exploratory analysis. We decided to retain the majority of the attributes found in both the Netflix and IMDb datasets since our research aims to identify which factors most strongly influence viewership. Removing such attributes could limit the scope of our findings later on. \n",
    "\n",
    "Much of the data cleaning that was performed using SQL. Our first task involved merging the IMDb datasets together, as title.ratings.tsv only contains ratings corresponding to an ID and not a film title while title.basics.tsv includes the corresponding film title along with their respective IDs. By performing a SQL join on these two datasets, we obtained a single dataframe containing both film titles and their ratings. \n",
    "\n",
    "The second task was to combine all of Netflix’s semi-annual engagement datasets into a master dataset. Using the UNION ALL operator in SQL, we were able to successfully retain all instances from all four datasets. During this process, we also noticed that some Netflix films contained titles in their original languages which would be a problem when merging with the IMDb dataset that only has titles in English. To ensure consistent title matching, we cleaned up the film titles by removing the extra info that followed // by simply replacing a part of the title string with an empty string. \n",
    "\n",
    "Once that was done, we performed a left join between our Netflix and IMDb datasets using the shared column Title. This join type allowed us to keep all instances of the Netflix dataset, even those without corresponding IMDb data, so our dataset wouldn’t be too limited to only titles that are matched. Additionally, we created two dummy variables, Is_Global and Successful, to support our exploratory analysis. The Is_Global variable was encoded as 1 for films globally available and 0 for unavailable. The boolean column, Successful, that marked whether a film had more than 50 million views was created and then converted into a binary variable with 1 being successful and 0 for not successful. \n",
    "\n",
    "All raw source data can be found on the Github folder which can also be accessed here. \n",
    "https://github.com/xuanlianliu/Data_Science_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96787bc1-84ed-4677-930a-97e1ad30cd54",
   "metadata": {},
   "source": [
    "# __Data Limitations:__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14aef76-3301-47f9-827d-601cc19ab155",
   "metadata": {},
   "source": [
    "There were several limitations we discovered in our dataset. Upon cleaning, we noticed that many of the films were missing release dates, making us reconsider whether this variable should be used to analyze and predict viewership. Our dataset also only covers a 2.5-year period worth of Netflix data spanning from January 2023 to June 2025. This short timeframe might make it hard for us to generalize our findings to other years and their films and may affect the accuracy of our predictive models. We also experienced limitations from the ratings dataset. Most importantly, not every Netflix title matches with an IMDb rating, so there are many missing ratings. This harms our predictions of viewership based on ratings because many films are missing from the analysis. We also observed variation in the number of votes that determined each film’s IMDb rating. Films with a smaller number of votes could skew the rating, which could ultimately affect our analysis of how ratings correlate with viewership. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94654b-100e-4171-95ce-d9f71ca2716b",
   "metadata": {},
   "source": [
    "# __Exploratory Data Analysis:__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8bd88-8fd3-4cd1-95a3-614e79ddbdbf",
   "metadata": {},
   "source": [
    "### Movie Viewership based on Ratings and Global Availability:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3200ca9-812e-4114-a1d3-559a5d7efc6d",
   "metadata": {},
   "source": [
    "Linear Regression (x - ratings, y - viewership), Avg Views on each rating as bar graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d504ad-e75c-4a10-977a-45438618b869",
   "metadata": {},
   "source": [
    "To help gauge whether or not there is a relationship between Views and Ratings and the relationship between Views and Global Availability, we need a series of graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa550e-ad46-491c-954e-fb64faf2c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot analysis on the number of views for a movie based on the ratings and global availability\n",
    "plot = sns.scatterplot(netflix_ratings_df, \n",
    "             x='averageRating', y='Views', marker='o', hue=\"Global\")\n",
    "plt.title(\"Number of Views based on Ratings\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786438a-eb4e-4167-b38d-17833fb8df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot analysis on the hours viewed for a movie based on the ratings and global availability\n",
    "plot = sns.scatterplot(netflix_ratings_df, \n",
    "             x='averageRating', y='Hours_Viewed', marker='o', hue=\"Global\")\n",
    "plt.title(\"Hours Viewed based on Ratings\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044cbf5-8b2a-43a7-9d72-dc7a1568286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line graph on the number of views for a movie based on the ratings and global availability\n",
    "\n",
    "plot = sns.lineplot(netflix_ratings_df, \n",
    "             x='averageRating', y='Views', hue=\"Global\")\n",
    "plt.title(\"Number of Views based on Ratings\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd42ad4-cfec-44a6-ae8f-f8fb66414e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line graph on the number of hours viewed for a movie based on the ratings and global availability\n",
    "plot = sns.lineplot(netflix_ratings_df, \n",
    "             x='averageRating', y='Hours_Viewed', hue=\"Global\")\n",
    "plt.title(\"Hours Viewed based on Ratings\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01cbdf-4924-4380-b346-44874dfa4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression analysis on ratings to predict view counts\n",
    "no_nan_df = netflix_ratings_df.copy().dropna()\n",
    "x = no_nan_df[['averageRating']].values\n",
    "y = no_nan_df[['Views']].values\n",
    "\n",
    "ratings_model = LinearRegression().fit(x,y)\n",
    "print(\"The model's slope is: \" + str(round(float(ratings_model.coef_[0][0]),2)))\n",
    "print(\"The model's intercept is: \" + str(round(float(ratings_model.intercept_[0]),2)))\n",
    "\n",
    "no_nan_df[\"View_Pred\"] = ratings_model.predict(x)\n",
    "plot2 = sns.scatterplot(no_nan_df, \n",
    "             x='averageRating', y='View_Pred')\n",
    "plt.title(\"View Predictions Based on Ratings\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c25843-4e90-4cd1-ab56-64293e5cc60a",
   "metadata": {},
   "source": [
    "### Movie Viewership over Time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18466848-7924-4a8f-a5f4-121ed9c06041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the total views per each time period\n",
    "netflix_summarized= duckdb.sql(\"\"\"SELECT Time_Period,\n",
    "SUM(Views) as Total_Views,\n",
    "FROM netflix_ratings_df\n",
    "WHERE Views IS NOT NULL\n",
    "GROUP BY Time_Period\n",
    "ORDER BY Time_Period ASC\"\"\").df()\n",
    "\n",
    "#check the new dataframe\n",
    "#print(netflix_summarized) #2021 H1 is gone. I think it has needs more cleaning above\n",
    "\n",
    "#graph the views over time with a scatterplot\n",
    "viewership_plot= plt.plot(netflix_summarized['Time_Period'], netflix_summarized['Total_Views'], 'o')\n",
    "plt.title(\"Views over Time\")\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Viewership')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2229ec83-f28a-4953-a177-ecf67cfe11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Views  Is_Global  averageRating  numVotes\n",
      "Views          1.000000   0.156080       0.027113  0.049239\n",
      "Is_Global      0.156080   1.000000      -0.015257 -0.095387\n",
      "averageRating  0.027113  -0.015257       1.000000  0.161030\n",
      "numVotes       0.049239  -0.095387       0.161030  1.000000\n"
     ]
    }
   ],
   "source": [
    "corr_df = netflix_ratings_df[['Views', 'Is_Global', 'averageRating', 'numVotes']].corr()\n",
    "print(corr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac19d48b-1599-43b4-86bd-3d2118f90f0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m x = netflix_ratings_df[\u001b[33m'\u001b[39m\u001b[33maverageRating\u001b[39m\u001b[33m'\u001b[39m].values.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m      2\u001b[39m y = netflix_ratings_df[\u001b[33m'\u001b[39m\u001b[33mSuccessful\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model.coef_[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m model.intercept_[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\2950\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\2950\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1256\u001b[39m check_classification_targets(y)\n\u001b[32m   1257\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\2950\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\2950\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\2950\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\2950\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\2950\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "#fitting a logistic regression model that predicts if movie is succesfful given average ratings\n",
    "x = netflix_ratings_df['averageRating'].values.reshape(-1, 1)\n",
    "y = netflix_ratings_df['Successful']\n",
    "model = LogisticRegression().fit(x,y)\n",
    "\n",
    "model.coef_[0][0]\n",
    "model.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32329d-efcb-4bb2-9223-2da37cbab202",
   "metadata": {},
   "source": [
    "# __Questions for Reviewers:__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63417b-f47a-40a1-8c18-98038bda08ea",
   "metadata": {},
   "source": [
    "How can we improve the merge of the netflix films and the IMDb ratings? Currently, we are joining by title after removing the foreign language part of the netflix titles. There is certainly variation in the way that films are listed across the two sources, which causes us to have more missing ratings than what exists.\n",
    "\n",
    "What are some other types of graphs we can make to explore our research question?\n",
    "\n",
    "Is the linear regression between ratings and viewership appropriate? One consideration is that someone would only rate the film after they watch it. Therefore, is it possible for ratings be on the x-axis while viewership is on the y-axis?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
